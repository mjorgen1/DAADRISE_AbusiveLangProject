{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.stem.cistem import Cistem\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import warnings\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import random \n",
    "import os\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Raw data\n",
    "train_data = pd.read_csv(\"/home/mackenzie/Downloads/GermanTrainingData.txt\", sep='\\t', names=['tweet', 'coarse', 'labels'])\n",
    "test_data = pd.read_csv(\"/home/mackenzie/Downloads/GermanTestingData.txt\", sep='\\t', names=['tweet', 'coarse', 'labels'])\n",
    "df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "del train_data\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "\n",
    "other_exclusions = [\"lbr\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = Cistem()\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-ßA-ß]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-ßA-ß]+\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-ßA-ß.,!?]+\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=3000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['all', 'ber', 'dami', 'dasselb', 'demselb', 'denselb', 'derselb', 'dess', 'desselb', 'dieselb', 'dor', 'etwa', 'eur', 'f', 'geg', 'hatt', 'hrend', 'jed', 'jen', 'jetz', 'k', 'mach', 'manch', 'nich', 'nne', 'nnt', 'ohn', 'r', 'rde', 'selb', 'solch', 'son', 'sond', 'w', 'welch', 'werd', 'wes', 'woll', 'zwisch'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=2000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-ßA-ß]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = TextBlob(\"tweet\").sentiment\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words, lang='de_DE')\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment[0],twitter_objs[2], twitter_objs[1], twitter_objs[0]]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "# Changing string labels to numeric labels\n",
    "def string_to_numeric(x):\n",
    "    if x == 'OTHER' or x == 'PROFANITY':\n",
    "        return 0\n",
    "    if x == 'INSULT':\n",
    "        return 1\n",
    "    if x == 'ABUSE':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"sentiment\", \"num_hashtags\", \"num_mentions\", \"num_urls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8407, 3918)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['labels'].apply(string_to_numeric)\n",
    "X.columns = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [3914 3917] are constant.\n",
      "  UserWarning)\n",
      "/home/mackenzie/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate Selection features found, use getUnivariateData() to get the features\n",
      "                   Specs      Score\n",
      "613                 dumm  64.461365\n",
      "2450          strunzdumm  55.880652\n",
      "1703                mosl  55.345552\n",
      "587               dophil  54.787880\n",
      "2194           schmarotz  53.819002\n",
      "1909                pack  52.935005\n",
      "1906            p dophil  52.840050\n",
      "1658              merkel  52.572181\n",
      "1241             invasor  49.851795\n",
      "1256               islam  48.541379\n",
      "170                asyla  47.059928\n",
      "1712             murksel  45.142919\n",
      "1854              npress  45.073805\n",
      "1466            l npress  43.138647\n",
      "2629              vasall  41.965389\n",
      "2173             scheiss  41.374236\n",
      "2223      schulabschluss  38.508092\n",
      "2728           volksverr  35.600844\n",
      "815               fettig  34.680448\n",
      "816          fettig haar  34.563703\n",
      "2195         schmarotz p  34.354193\n",
      "2196  schmarotz p dophil  34.354193\n",
      "2227              schulz  33.965406\n",
      "22              abschaum  33.503491\n",
      "1905                   p  33.437801\n",
      "2786                 wdr  33.264589\n",
      "998                   gr  30.828795\n",
      "588      dophil denunzia  30.502020\n",
      "1907   p dophil denunzia  30.502020\n",
      "2350       spd schmarotz  30.502020\n",
      "...                  ...        ...\n",
      "100                angab   0.506572\n",
      "149            arbeitgeb   0.506572\n",
      "158                armee   0.506572\n",
      "183             aufh ren   0.506572\n",
      "204               ausseh   0.506572\n",
      "342              betreib   0.506572\n",
      "444              cdu spd   0.506572\n",
      "467      chtlingspolitik   0.506572\n",
      "494           daf r sorg   0.506572\n",
      "634                  ehr   0.506572\n",
      "653               einsch   0.506572\n",
      "663                elend   0.506572\n",
      "673                enorm   0.506572\n",
      "703                erheb   0.506572\n",
      "762           f r europa   0.506572\n",
      "766            f r immer   0.506572\n",
      "831   fl chtlingspolitik   0.506572\n",
      "929              geh gut   0.506572\n",
      "950                 gend   0.506572\n",
      "963              get tet   0.506572\n",
      "995                googl   0.506572\n",
      "1028            grundlag   0.506572\n",
      "1140               hiess   0.506572\n",
      "1699           moralisch   0.506572\n",
      "1751           nder nich   0.506572\n",
      "1808              nich h   0.506572\n",
      "1921              patrio   0.506572\n",
      "1934               pferd   0.506572\n",
      "1947        politik nich   0.506572\n",
      "1952            polnisch   0.506572\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "Index(['dumm', 'strunzdumm', 'mosl', 'dophil', 'schmarotz', 'pack', 'p dophil',\n",
      "       'merkel', 'invasor', 'islam',\n",
      "       ...\n",
      "       'googl', 'grundlag', 'hiess', 'moralisch', 'nder nich', 'nich h',\n",
      "       'patrio', 'pferd', 'politik nich', 'polnisch'],\n",
      "      dtype='object', length=3000)\n"
     ]
    }
   ],
   "source": [
    "# Univariate feature selection from sklearn\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=3000)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "# concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print('Univariate Selection features found, use getUnivariateData() to get the features')\n",
    "# Extract the top n features\n",
    "uni_selected_feat = featureScores.nlargest(3000,'Score')\n",
    "print(uni_selected_feat) # print out the top n features selected\n",
    "\n",
    "# Saving the top n features to a data frame\n",
    "#print(a.iloc[0].name) # how to get the column # for the ith feature\n",
    "#print(a.iloc[0][0]) # how to get the header column\n",
    "top_univariate_features = pd.DataFrame()\n",
    "for i in range(0, 3000):\n",
    "    curr_column_vals = X.iloc[:, uni_selected_feat.iloc[i].name]\n",
    "    curr_column_name = uni_selected_feat.iloc[i][0]\n",
    "    top_univariate_features[curr_column_name] = curr_column_vals\n",
    "\n",
    "X = pd.DataFrame(top_univariate_features)\n",
    "X.columns = top_univariate_features.columns\n",
    "print(X.columns)\n",
    "feature_names = top_univariate_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8407\n",
      "8407\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_train = y_train.to_frame(name='labels')\n",
    "y_test = y_test.to_frame(name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom imblearn.over_sampling import SMOTE\\nsmote = SMOTE('minority')\\n# convert back to pandas dataframe\\n#X_train_pd = X_train.as_data_frame()\\n#X_train_pd.columns = feature_names\\n#y_train_pd = y_train.as_data_frame()\\nX_sm, y_sm = smote.fit_resample(X_train, y_train)\\nprint(X_sm.shape)\\nprint(y_sm.shape)\\nX_train = pd.DataFrame(X_sm)\\nX_train.columns = feature_names\\ny_train = pd.DataFrame(y_sm)\\ny_train.columns = ['labels']\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOTE -- not the best technique because oversampling for text doesn't seem great\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE('minority')\n",
    "# convert back to pandas dataframe\n",
    "#X_train_pd = X_train.as_data_frame()\n",
    "#X_train_pd.columns = feature_names\n",
    "#y_train_pd = y_train.as_data_frame()\n",
    "X_sm, y_sm = smote.fit_resample(X_train, y_train)\n",
    "print(X_sm.shape)\n",
    "print(y_sm.shape)\n",
    "X_train = pd.DataFrame(X_sm)\n",
    "X_train.columns = feature_names\n",
    "y_train = pd.DataFrame(y_sm)\n",
    "y_train.columns = ['labels']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Try undersampling from imblearn\\nfrom collections import Counter\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.under_sampling import RandomUnderSampler\\nrus = RandomUnderSampler(random_state=42)\\nX_res, y_res = rus.fit_resample(X_train, y_train)\\nX_train = pd.DataFrame(X_res)\\nX_train.columns = feature_names\\ny_train = pd.DataFrame(y_res)\\ny_train.columns = ['labels']\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Try undersampling from imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "X_train = pd.DataFrame(X_res)\n",
    "X_train.columns = feature_names\n",
    "y_train = pd.DataFrame(y_res)\n",
    "y_train.columns = ['labels']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>44 mins 57 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Vienna</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>15 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mackenzie_h9xlwq</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.728 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         44 mins 57 secs\n",
       "H2O cluster timezone:       Europe/Vienna\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.1\n",
       "H2O cluster version age:    15 days\n",
       "H2O cluster name:           H2O_from_python_mackenzie_h9xlwq\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.728 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.1 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# BUG: when we convert to h2o dataframe an extra row is added\n",
    "X_train = h2o.H2OFrame(X_train)\n",
    "y_train = h2o.H2OFrame(y_train)\n",
    "X_test = h2o.H2OFrame(X_test)\n",
    "y_test = h2o.H2OFrame(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.na_omit() # pandas-->h2o results in an error where a nan row is added to X data, so this deals with that\n",
    "X_test = X_test.na_omit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7566\n",
      "7566\n",
      "841\n",
      "841\n"
     ]
    }
   ],
   "source": [
    "# Double check X and y have same number of rows\n",
    "print(len(y_train))\n",
    "print(len(X_train))\n",
    "print(len(y_test))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7566, 3000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the train and test data sets\n",
    "# now convert tweet vecs and labels to a pandas dataframe and back to h2o dataframe\n",
    "train = X_train.cbind(y_train)\n",
    "test = X_test.cbind(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h2o.download_all_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more on data prep\n",
    "x = train.columns         # x: A list/vector of predictor column names or indexes. \n",
    "                          # This argument only needs to be specified if the user wants to exclude columns from the \n",
    "                          # set of predictors. If all columns (other than the response) should be used in prediction, \n",
    "                          # then this does not need to be set.\n",
    "\n",
    "y = \"labels\"              # This argument is the name (or index) of the response column\n",
    "x.remove(y)\n",
    "\n",
    "# need to set train and test\n",
    "train[y] = train[y].asfactor()\n",
    "test[y] = test[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████"
     ]
    }
   ],
   "source": [
    "# now the AUTO-ML piece comes in\n",
    "aml = H2OAutoML(max_runtime_secs=1800) #sort_metric=auc, max_runtime_secs=10800, class_sampling_factors = sample_factors, max_models=?, balance_classes=True, class_sampling_factors = sample_factors\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h2o.download_all_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The leader model is stored here\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions!\n",
    "preds = aml.predict(test)\n",
    "print(preds)\n",
    "var = preds[\"predict\"].cbind(test[y])\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder to hold statistics for a run\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "# create a random folder number to reference the directory\n",
    "rand = random.randint(0, 1000)*random.randint(0,10)\n",
    "directory = '/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/TestResults/Test_' + str(rand)\n",
    "print('The results for this run will be stored in ' + directory)\n",
    "createFolder(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and results!\n",
    "y_test = h2o.as_list(test[y], use_pandas=True)\n",
    "y_pred = h2o.as_list(preds[\"predict\"])\n",
    "print(\"Confusion Matrix: \")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy Score: \")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score: \")\n",
    "print(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall: \")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Other','Insult','Abuse']\n",
    "confusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True categories',fontsize=14)\n",
    "plt.xlabel(r'Predicted categories',fontsize=14)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.savefig(directory + '/recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the classification report info\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "f= open(directory+'/classification_report.txt',\"w+\")\n",
    "f.write(metrics.classification_report(y_test, y_pred))\n",
    "f.write(str(metrics.confusion_matrix(y_test,y_pred)))\n",
    "f.write('\\n')\n",
    "f.write('accuracy: '+ str(metrics.accuracy_score(y_test, y_pred)))\n",
    "f.write('\\n')\n",
    "f.write('f1-score' + str(metrics.f1_score(y_test, y_pred, average=\"weighted\")))\n",
    "f.write('\\n')\n",
    "f.write('leader model: ')\n",
    "f.write(str(aml.leader))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
