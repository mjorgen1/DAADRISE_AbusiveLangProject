{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autosklearn.classification\n",
    "#import sklearn.model_selection\n",
    "#import sklearn.datasets\n",
    "#import sklearn.metrics\n",
    "import pandas as pd\n",
    "import re\n",
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english')) # can we take out now that its been preprocessed?\n",
    "new_stopwords = ['rt', 'co', 'http', 'u', 'got', 'get']\n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for H2o Word 2 Vec\n",
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time\n",
    "train_data = h2o.upload_file(\"/home/mackenzie/Downloads/EnglishCleanedTrainingData.csv\") # check if path correct\n",
    "test_data = h2o.upload_file(\"/home/mackenzie/Downloads/EnglishCleanedTestingData.csv\") # check if path correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 Vec process, could turn into a function or nah?\n",
    "print(\"Break train tweets into sequence of words\")\n",
    "train_words = tokenize(train_data[\"tweet\"])\n",
    "\n",
    "print(\"Break test tweets into sequence of words\")\n",
    "test_words = tokenize(test_data[\"tweet\"])\n",
    "\n",
    "print(\"Build word2vec model for train\")\n",
    "w2v_model_one = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "w2v_model_one.train(training_frame=train_words)\n",
    "\n",
    "print(\"Build word2vec model for test\")\n",
    "w2v_model_two = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "w2v_model_two.train(training_frame=test_words)\n",
    "\n",
    "print(\"Calculate a vector for each train tweet\")\n",
    "tweet_vecs_train = w2v_model_one.transform(train_words, aggregate_method=\"AVERAGE\")\n",
    "\n",
    "print(\"Calculate a vector for each test tweet\")\n",
    "tweet_vecs_test = w2v_model_two.transform(test_words, aggregate_method=\"AVERAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word 2 vec back to pandas for autosklearn later\n",
    "train_tweets_pd = h2o.as_list(tweet_vecs_train, use_pandas=True)\n",
    "train_labels_pd = h2o.as_list(train_data[\"labels\"], use_pandas=True)\n",
    "test_tweets_pd = h2o.as_list(tweet_vecs_test, use_pandas=True)\n",
    "test_labels_pd = h2o.as_list(test_data[\"labels\"], use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams function\n",
    "def time4ngrams(s, n):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s) # lowercases all uppercase words, takes out punctuation, and allows for numbers\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    output = list(ngrams(tokens, n)) # the higher the number the smaller the ngrams list\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running ngrams for training data\n",
    "tweets_train = pd.Dataframe(train_data[\"tweet\"])\n",
    "ngrammed_tweets_train = pd.Dataframe(column=\"tweet\") # might not need the parameter\n",
    "n = 3 # if we want multiple ngrams then will need to add a loop\n",
    "for t in tweets_train:\n",
    "    ngrammed_tweets_train[\"tweet\"].append(time4ngrams(t, n))\n",
    "print(type(ngrammed_tweets_train)) # make sure it's in pandas\n",
    "\n",
    "tweets_test = pd.Dataframe(test_data[\"tweet\"])\n",
    "ngrammed_tweets_test = pd.Dataframe(column=\"tweet\") # might not need the parameter\n",
    "n = 3 # if we want multiple ngrams then will need to add a loop\n",
    "for t in tweets_test:\n",
    "    ngrammed_tweets_test[\"tweet\"].append(time4ngrams(t, n))\n",
    "print(type(ngrammed_tweets_test)) # make sure it's in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tfidf on the ngrams\n",
    "vectorizer = TfidfVectorizer()\n",
    "got_tfidf = vectorizer.fit_transform(ngrammed_tweets) # the input for tfidf would be the ngrams from above?\n",
    "tfidf = pd.DataFrame(got_tfidf.toarray())\n",
    "tfidf.columns = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then combine all columns to train features and test features\n",
    "\n",
    "# once run and no errors, do the same for german autosklearn and then do it for h2o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
