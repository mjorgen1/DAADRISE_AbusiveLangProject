{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english')) \n",
    "new_stopwords = ['rt']\n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>5 hours 54 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Vienna</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>19 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mackenzie_j6zlxj</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.638 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         5 hours 54 mins\n",
       "H2O cluster timezone:       Europe/Vienna\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    19 days\n",
       "H2O cluster name:           H2O_from_python_mackenzie_j6zlxj\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.638 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.1 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# prepare data for h2o use\n",
    "# check if paths are correct\n",
    "filepath_train = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTrainingData (1).csv\"\n",
    "filepath_test = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTestingData (1).csv\"\n",
    "train_data = h2o.upload_file(filepath_train) \n",
    "test_data = h2o.upload_file(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for H2o Word2Vec\n",
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenize(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break train into sequence of words\n",
      "Build word2vec model for train\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n",
      "Break test into sequence of words\n",
      "Build word2vec model for test\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec generation\n",
    "vecs_train = h2o_w2vec(train_data['tweet'], 'train')\n",
    "train_labels = train_data[\"labels\"]\n",
    "vecs_test = h2o_w2vec(test_data['tweet'], 'test')\n",
    "test_labels = test_data[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator(filepath, textcol=0, skipheader=True): # might want to make false?\n",
    "    with open(filepath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        if skipheader:\n",
    "            next(reader, None)\n",
    "        for row in reader:\n",
    "            yield row[textcol]\n",
    "            \n",
    "def ngrams(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = CountVectorizer(ngram_range=(min_n, max_n), stop_words=set(STOP_WORDS))\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col)) # for our purposes col=0\n",
    "    # print(X.toarray()) -- get a memory error when i try and run this\n",
    "    #print(\"Testing ngram generation for \" + str)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    #X_arr = X.toarray() \n",
    "    #print(len(X_arr))\n",
    "    #print(\"Testing ngram vectors\" + str)\n",
    "    return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing ngram generation for train\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- getting memory error when convertint to pandas\n",
    "\n",
    "ngram_train_freq = ngrams(5, 5, 'train', filepath_train, 0)\n",
    "#ngram_test_freq = ngrams(2, 2, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF function\n",
    "def tfidf(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(min_n, max_n))\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col))\n",
    "    # Testing the TFIDF value + ngrams:\n",
    "    #print(X.toarray()) -- get a memory error when i try and run this\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Generation -- getting memory error when convertint to pandas\n",
    "\n",
    "tfidf_train = tfidf(5, 5, 'train', filepath_train, 0)\n",
    "#tfidf_test = tfidf(2, 2, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
