{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import readability\n",
    "import os\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preparations\n",
    "os.chdir('/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction') #'C:\\\\Users\\\\mikec\\\\Documents'\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "new_stopwords = ['rt'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 12,
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    #bv_vocab_addition = [str(col) + '_nc' for col in enumerate(bv_vocab)]\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
<<<<<<< HEAD
    "    bv_data.columns = [str(col) + '_x' for col in bv_data.columns]\n",
=======
    "    bv_data.columns = [col+ '_nw' for col in bv_data.columns]\n",
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
    "    return bv_data\n",
    "\n",
    "# Char-NGrams Freq function\n",
    "def char_ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing char-ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [col+ '_nc' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col+ '_tw' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "# Char-TFIDF function\n",
    "def char_tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing char-tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col+ '_tc' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def is_retweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if 'RT' in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_elongated(text):\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def number_of_slangs(text):\n",
    "    slang_counter = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slang_counter += 1\n",
    "    return slang_counter\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    return len(re.findall(r\"&#\\S+\", text))\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, irt, nom, noh, nol, noem, nt, noel, nos = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        irt.append(is_retweet(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "        noel.append(number_of_elongated(tweets[i]))\n",
    "        nos.append(number_of_slangs(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['retweet'] = irt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of elongated'] = noel\n",
    "    features['number of slangs'] = nos\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Textstat score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    text = re.sub(r'\\&#S+', '', text)\n",
    "    return text\n",
    "\n",
    "def textstatScores(tweets, str):\n",
    "    print(\"Completing the text stat scores extraction for \" + str)\n",
    "    fkg, ari, cli, fre, gfi, lix, si, rix, dci = [], [], [], [], [], [], [], [], []\n",
    "    cpw, spw, wps, ttr, c, s, w, wt, lw, cw, cwdc = [], [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        # Readability\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        tweet = remove_emojis(tweet)\n",
    "        measures = readability.getmeasures(tweet, lang='en')\n",
    "        fkg.append(measures['readability grades']['Kincaid'])\n",
    "        ari.append(measures['readability grades']['ARI'])\n",
    "        cli.append(measures['readability grades']['Coleman-Liau'])\n",
    "        fre.append(measures['readability grades']['FleschReadingEase'])\n",
    "        gfi.append(measures['readability grades']['GunningFogIndex'])\n",
    "        lix.append(measures['readability grades']['LIX'])\n",
    "        si.append(measures['readability grades']['SMOGIndex'])\n",
    "        rix.append(measures['readability grades']['RIX'])\n",
    "        dci.append(measures['readability grades']['DaleChallIndex'])\n",
    "    \n",
    "        # Sentence\n",
    "        cpw.append(measures['sentence info']['characters_per_word'])\n",
    "        spw.append(measures['sentence info']['syll_per_word'])\n",
    "        wps.append(measures['sentence info']['words_per_sentence'])\n",
    "        ttr.append(measures['sentence info']['type_token_ratio'])\n",
    "        c.append(measures['sentence info']['characters'])\n",
    "        s.append(measures['sentence info']['syllables'])\n",
    "        w.append(measures['sentence info']['words'])\n",
    "        wt.append(measures['sentence info']['wordtypes'])\n",
    "        lw.append(measures['sentence info']['long_words'])\n",
    "        cw.append(measures['sentence info']['complex_words'])\n",
    "        cwdc.append(measures['sentence info']['complex_words_dc'])\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    # Readability\n",
    "    features['Kincaid'] = fkg\n",
    "    features['ARI'] = ari\n",
    "    features['Coleman-Liau'] = cli\n",
    "    features['FleschReadingEase'] = fre\n",
    "    features['GunningFogIndex'] = gfi\n",
    "    features['LIX'] = lix\n",
    "    features['SMOGIndex'] = si\n",
    "    features['RIX'] = rix\n",
    "    features['DaleChallIndex'] = dci\n",
    "    # Sentence\n",
    "    features['Characters per word'] = cpw\n",
    "    features['Syllables per word'] = spw\n",
    "    features['Words per sentence'] = wps\n",
    "    features['Type toke ratio'] = ttr\n",
    "    features['Characters'] = c\n",
    "    features['Syllables'] = s\n",
    "    features['Words'] = w\n",
    "    features['Wordtypes'] = wt\n",
    "    features['Long words'] = lw\n",
    "    features['Complex words'] = cw\n",
    "    features['Complex words dc'] = cwdc\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Checking whether there is an H2O instance running at http://localhost:54321 ",
      ".",
      ".",
      ".",
      ".",
      ".",
      " not found.",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mH2OServerError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-25cc1a066fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create an h2o instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mh2o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\h2o.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m                                      \u001b[0mverify_ssl_certificates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_ssl_certificates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                                      \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                                      _msgs=(\"Checking whether there is an H2O instance running at {url} \",\n\u001b[0m\u001b[0;32m    267\u001b[0m                                             \"connected.\", \"not found.\"))\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mH2OConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[1;31m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;31m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36m_test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcld\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud_healthy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cluster reports unhealthy status\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcld\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsensus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cluster cannot reach consensus\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mH2OServerError\u001b[0m: Cluster reports unhealthy status"
     ],
     "ename": "H2OServerError",
     "evalue": "Cluster reports unhealthy status",
     "output_type": "error"
=======
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>3 mins 29 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Vienna</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>26 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mackenzie_zl082s</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.896 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         3 mins 29 secs\n",
       "H2O cluster timezone:       Europe/Vienna\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    26 days\n",
       "H2O cluster name:           H2O_from_python_mackenzie_zl082s\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.896 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.1 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
>>>>>>> 28dee4026838dc337c929e613f0bf6130a6b6a4c
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_train = \"EnglishCleanedTrainingData.csv\"\n",
    "filepath_test = \"EnglishCleanedTestingData.csv\"\n",
    "train_data_h2o = h2o.upload_file(filepath_train) \n",
    "test_data_h2o = h2o.upload_file(filepath_test)\n",
    "train_data = pd.read_csv(filepath_train)\n",
    "test_data = pd.read_csv(filepath_test)\n",
    "train_labels = train_data[\"labels\"]\n",
    "test_labels = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Break train into sequence of words",
      "\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%",
      "\n",
      "Build word2vec model for train",
      "\n",
      "word2vec Model Build progress: |",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "██",
      "█",
      "█",
      "██",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "██",
      "█",
      "██",
      "█",
      "█",
      "███",
      "█",
      "█",
      "█",
      "█",
      "███| 100%",
      "\n",
      "Break test into sequence of words",
      "\n",
      "Parse progress: |",
      "█████████████████████████████████████████████████████████| 100%",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data_h2o['cleaned_tweet'], 'train')).as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data_h2o['cleaned_tweet'], 'test')).as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train = ngrams(train_data['cleaned_tweet'], 1, 3, 'train') \n",
    "ngram_test = ngrams(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# Char-NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "char_ngram_train = char_ngrams(train_data['cleaned_tweet'], 2, 5, 'train') \n",
    "char_ngram_test = char_ngrams(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n",
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(train_data['cleaned_tweet'], 1, 3, 'train')\n",
    "tfidf_test = tfidf(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# TFIDF Generation -- NOTE no header column\n",
    "char_tfidf_train = char_tfidf(train_data['cleaned_tweet'], 2, 5, 'train')\n",
    "char_tfidf_test = char_tfidf(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the sentiment analysis for train\n",
      "Completing the sentiment analysis for test\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer(train_data['cleaned_tweet'], 'train') \n",
    "sentiment_test = sentimentAnalyzer(test_data['cleaned_tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the liguistic feature extraction for train\n",
      "Completing the liguistic feature extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_train = linguisticFeatures(train_data['tweet'], 'train')\n",
    "linguistic_test = linguisticFeatures(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the text stat scores extraction for train\n",
      "Completing the text stat scores extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Textstat scores\n",
    "textstat_train = textstatScores(train_data['tweet'], 'train')\n",
    "textstat_test = textstatScores(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating in process\n",
      "Did it!\n"
     ]
    }
   ],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "training_data = pd.concat([ngram_train, char_ngram_train, tfidf_train, char_tfidf_train, sentiment_train, \n",
    "                           linguistic_train, textstat_train, train_labels], axis=1) # took out vecs_train\n",
    "print(\"Did it!\")\n",
    "\n",
    "#testing_data = pd.concat([vecs_test, ngram_test, tfidf_test, sentiment_test, test_labels], axis=1) \n",
    "#export_csv2 = testing_data.to_csv('english_test_data.csv', index = None, header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    return training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
