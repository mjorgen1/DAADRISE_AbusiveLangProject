{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textstat\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Preparations\n",
    "with open('C:\\\\Users\\\\mikec\\\\Documents\\\\slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "new_stopwords = ['rt'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=10000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=10000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    return tv_data\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def is_retweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if 'RT' in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_elongated(text):\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def number_of_slangs(text):\n",
    "    slang_counter = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slang_counter += 1\n",
    "    return slang_counter\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    return len(re.findall(r\"&#\\S+\", text))\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, irt, nom, noh, nol, noem, nt, noel, nos = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        irt.append(is_retweet(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "        noel.append(number_of_elongated(tweets[i]))\n",
    "        nos.append(number_of_slangs(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['retweet'] = irt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of elongated'] = noel\n",
    "    features['number of slangs'] = nos\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Textstat score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def textstatScores(tweets, str):\n",
    "    print(\"Completing the text stat scores extraction for \" + str)\n",
    "    fre, fkg, fs, si, ari, cli, lwf, dcrs = [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        fre.append(textstat.flesch_reading_ease(tweet))\n",
    "        fkg.append(textstat.flesch_kincaid_grade(tweet))\n",
    "        fs.append(textstat.gunning_fog(tweet))\n",
    "        si.append(textstat.smog_index(tweet))\n",
    "        ari.append(textstat.automated_readability_index(tweet))\n",
    "        cli.append(textstat.coleman_liau_index(tweet))\n",
    "        lwf.append(textstat.linsear_write_formula(tweet))\n",
    "        dcrs.append(textstat.dale_chall_readability_score(tweet))\n",
    "    features = pd.DataFrame()\n",
    "    features['flesch reading ease'] = fre\n",
    "    features['flesch kincaid grade'] = fkg\n",
    "    features['gunning fog'] = fs\n",
    "    features['smog index'] = si\n",
    "    features['automated readability index'] = ari\n",
    "    features['coleman liau index'] = cli\n",
    "    features['linsear write formula'] = lwf\n",
    "    features['dale chall readability score'] = dcrs\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ",
      ".",
      " connected.",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "--------------------------  ------------------------------------------\nH2O cluster uptime:         31 mins 24 secs\nH2O cluster timezone:       Europe/Berlin\nH2O data parsing timezone:  UTC\nH2O cluster version:        3.24.0.5\nH2O cluster version age:    22 days\nH2O cluster name:           H2O_from_python_mikec_wj0miy\nH2O cluster total nodes:    1\nH2O cluster free memory:    203.1 Mb\nH2O cluster total cores:    8\nH2O cluster allowed cores:  8\nH2O cluster status:         locked, healthy\nH2O connection url:         http://localhost:54321\nH2O connection proxy:\nH2O internal security:      False\nH2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, Core V4\nPython version:             3.7.3 final\n--------------------------  ------------------------------------------",
      "text/html": "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n<td>31 mins 24 secs</td></tr>\n<tr><td>H2O cluster timezone:</td>\n<td>Europe/Berlin</td></tr>\n<tr><td>H2O data parsing timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O cluster version:</td>\n<td>3.24.0.5</td></tr>\n<tr><td>H2O cluster version age:</td>\n<td>22 days </td></tr>\n<tr><td>H2O cluster name:</td>\n<td>H2O_from_python_mikec_wj0miy</td></tr>\n<tr><td>H2O cluster total nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O cluster free memory:</td>\n<td>203.1 Mb</td></tr>\n<tr><td>H2O cluster total cores:</td>\n<td>8</td></tr>\n<tr><td>H2O cluster allowed cores:</td>\n<td>8</td></tr>\n<tr><td>H2O cluster status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O connection url:</td>\n<td>http://localhost:54321</td></tr>\n<tr><td>H2O connection proxy:</td>\n<td>None</td></tr>\n<tr><td>H2O internal security:</td>\n<td>False</td></tr>\n<tr><td>H2O API Extensions:</td>\n<td>Amazon S3, Algos, AutoML, Core V3, Core V4</td></tr>\n<tr><td>Python version:</td>\n<td>3.7.3 final</td></tr></table></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%",
      "\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_train = \"C:\\\\Users\\\\mikec\\\\Documents\\\\EnglishCleanedTrainingData.csv\"\n",
    "filepath_test = \"C:\\\\Users\\\\mikec\\\\Documents\\\\EnglishCleanedTestingData.csv\"\n",
    "train_data_h2o = h2o.upload_file(filepath_train) \n",
    "test_data_h2o = h2o.upload_file(filepath_test)\n",
    "train_data = pd.read_csv(filepath_train)\n",
    "test_data = pd.read_csv(filepath_test)\n",
    "train_labels = train_data[\"labels\"]\n",
    "test_labels = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Break train into sequence of words",
      "\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data_h2o['cleaned_tweet'], 'train')).as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data_h2o['cleaned_tweet'], 'test')).as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing ngram generation for train",
      "\n",
      "Completing ngram generation for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train = ngrams(train_data['cleaned_tweet'], 1, 3, 'train') \n",
    "ngram_test = ngrams(test_data['cleaned_tweet'], 1, 3, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing tfidf+ngram generation for train",
      "\n",
      "Completing tfidf+ngram generation for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(train_data['cleaned_tweet'], 1, 3, 'train')\n",
    "tfidf_test = tfidf(test_data['cleaned_tweet'], 1, 3, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the sentiment analysis for train",
      "\n",
      "Completing the sentiment analysis for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer(train_data['cleaned_tweet'], 'train') \n",
    "sentiment_test = sentimentAnalyzer(test_data['cleaned_tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the liguistic feature extraction for train",
      "\n",
      "Completing the liguistic feature extraction for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_train = linguisticFeatures(train_data['tweet'], 'train')\n",
    "linguistic_test = linguisticFeatures(test_data['tweet'], 'test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the text stat scores extraction for train",
      "\n",
      "Completing the text stat scores extraction for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Textstat scores\n",
    "textstat_train = textstatScores(train_data['tweet'], 'train')\n",
    "textstat_test = textstatScores(test_data['tweet'], 'test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(19808, 100)",
      "\n",
      "(19808, 10000)",
      "\n",
      "(19808, 10000)",
      "\n",
      "(19808, 1)",
      "\n",
      "(19808, 9)",
      "\n",
      "(19808, 8)",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(vecs_train.shape)\n",
    "print(ngram_train.shape)\n",
    "print(tfidf_train.shape)\n",
    "print(sentiment_train.shape)\n",
    "print(linguistic_train.shape)\n",
    "print(textstat_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "training_data = pd.concat([vecs_train, ngram_train, tfidf_train, sentiment_train, linguistic_train, textstat_train,\n",
    "                           train_labels], axis=1)\n",
    "export_csv = training_data.to_csv('english_train_data.csv', index = None, header=True, encoding='utf-8')\n",
    "print(\"Dit it!\")\n",
    "\n",
    "#testing_data = pd.concat([vecs_test, ngram_test, tfidf_test, sentiment_test, test_labels], axis=1) \n",
    "#export_csv2 = testing_data.to_csv('english_test_data.csv', index = None, header=True, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}