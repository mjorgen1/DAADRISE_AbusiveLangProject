{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textstat\n",
    "import os\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preparations\n",
    "os.chdir('C:\\\\Users\\\\mikec\\\\Documents')\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "new_stopwords = ['rt'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [str(col) + '_x' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# Char-NGrams Freq function\n",
    "def char_ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    return tv_data\n",
    "\n",
    "# Char-TFIDF function\n",
    "def char_tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    return tv_data\n",
    "\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def is_retweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if 'RT' in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_elongated(text):\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def number_of_slangs(text):\n",
    "    slang_counter = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slang_counter += 1\n",
    "    return slang_counter\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    return len(re.findall(r\"&#\\S+\", text))\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, irt, nom, noh, nol, noem, nt, noel, nos = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        irt.append(is_retweet(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "        noel.append(number_of_elongated(tweets[i]))\n",
    "        nos.append(number_of_slangs(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['retweet'] = irt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of elongated'] = noel\n",
    "    features['number of slangs'] = nos\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Textstat score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    text = re.sub(r'\\&#S+', '', text)\n",
    "    return text\n",
    "\n",
    "def textstatScores(tweets, str):\n",
    "    print(\"Completing the text stat scores extraction for \" + str)\n",
    "    fre, fkg, fs, si, ari, cli, lwf, dcrs = [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        tweet = remove_emojis(tweet)\n",
    "        fre.append(textstat.flesch_reading_ease(tweet))\n",
    "        fkg.append(textstat.flesch_kincaid_grade(tweet))\n",
    "        fs.append(textstat.gunning_fog(tweet))\n",
    "        si.append(textstat.smog_index(tweet))\n",
    "        ari.append(textstat.automated_readability_index(tweet))\n",
    "        cli.append(textstat.coleman_liau_index(tweet))\n",
    "        lwf.append(textstat.linsear_write_formula(tweet))\n",
    "        dcrs.append(textstat.dale_chall_readability_score(tweet))\n",
    "    features = pd.DataFrame()\n",
    "    features['flesch reading ease'] = fre\n",
    "    features['flesch kincaid grade'] = fkg\n",
    "    features['gunning fog'] = fs\n",
    "    features['smog index'] = si\n",
    "    features['automated readability index'] = ari\n",
    "    features['coleman liau index'] = cli\n",
    "    features['linsear write formula'] = lwf\n",
    "    features['dale chall readability score'] = dcrs\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ",
      ".",
      ".",
      ".",
      ".",
      ".",
      " not found.",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mH2OServerError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-25cc1a066fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create an h2o instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mh2o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\h2o.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m                                      \u001b[0mverify_ssl_certificates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_ssl_certificates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                                      \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                                      _msgs=(\"Checking whether there is an H2O instance running at {url} \",\n\u001b[0m\u001b[0;32m    267\u001b[0m                                             \"connected.\", \"not found.\"))\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mH2OConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[1;31m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;31m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikec\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h2o\\backend\\connection.py\u001b[0m in \u001b[0;36m_test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcld\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud_healthy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cluster reports unhealthy status\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcld\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsensus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cluster cannot reach consensus\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mH2OServerError\u001b[0m: Cluster reports unhealthy status"
     ],
     "ename": "H2OServerError",
     "evalue": "Cluster reports unhealthy status",
     "output_type": "error"
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_train = \"EnglishCleanedTrainingData.csv\"\n",
    "filepath_test = \"EnglishCleanedTestingData.csv\"\n",
    "train_data_h2o = h2o.upload_file(filepath_train) \n",
    "test_data_h2o = h2o.upload_file(filepath_test)\n",
    "train_data = pd.read_csv(filepath_train)\n",
    "test_data = pd.read_csv(filepath_test)\n",
    "train_labels = train_data[\"labels\"]\n",
    "test_labels = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Break train into sequence of words",
      "\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%",
      "\n",
      "Build word2vec model for train",
      "\n",
      "word2vec Model Build progress: |",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "██",
      "█",
      "█",
      "██",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "█",
      "██",
      "█",
      "██",
      "█",
      "█",
      "███",
      "█",
      "█",
      "█",
      "█",
      "███| 100%",
      "\n",
      "Break test into sequence of words",
      "\n",
      "Parse progress: |",
      "█████████████████████████████████████████████████████████| 100%",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data_h2o['cleaned_tweet'], 'train')).as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data_h2o['cleaned_tweet'], 'test')).as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing ngram generation for train",
      "\n",
      "Completing ngram generation for test",
      "\n",
      "Completing ngram generation for train",
      "\n",
      "Completing ngram generation for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train = ngrams(train_data['cleaned_tweet'], 1, 3, 'train') \n",
    "ngram_test = ngrams(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# Char-NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "char_ngram_train = char_ngrams(train_data['cleaned_tweet'], 2, 5, 'train') \n",
    "char_ngram_test = char_ngrams(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing tfidf+ngram generation for train",
      "\n",
      "Completing tfidf+ngram generation for test",
      "\n",
      "Completing tfidf+ngram generation for train",
      "\n",
      "Completing tfidf+ngram generation for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(train_data['cleaned_tweet'], 1, 3, 'train')\n",
    "tfidf_test = tfidf(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# TFIDF Generation -- NOTE no header column\n",
    "char_tfidf_train = char_tfidf(train_data['cleaned_tweet'], 2, 5, 'train')\n",
    "char_tfidf_test = char_tfidf(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the sentiment analysis for train",
      "\n",
      "Completing the sentiment analysis for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer(train_data['cleaned_tweet'], 'train') \n",
    "sentiment_test = sentimentAnalyzer(test_data['cleaned_tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the liguistic feature extraction for train",
      "\n",
      "Completing the liguistic feature extraction for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_train = linguisticFeatures(train_data['tweet'], 'train')\n",
    "linguistic_test = linguisticFeatures(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing the text stat scores extraction for train",
      "\n",
      "Completing the text stat scores extraction for test",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Textstat scores\n",
    "textstat_train = textstatScores(train_data['tweet'], 'train')\n",
    "textstat_test = textstatScores(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Concatenating in process",
      "\n",
      "Did it!",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "training_data = pd.concat([ngram_train, char_ngram_train, tfidf_train, char_tfidf_train, sentiment_train, \n",
    "                           linguistic_train, textstat_train, train_labels], axis=1) # took out vecs_train\n",
    "print(\"Did it!\")\n",
    "\n",
    "#testing_data = pd.concat([vecs_test, ngram_test, tfidf_test, sentiment_test, test_labels], axis=1) \n",
    "#export_csv2 = testing_data.to_csv('english_test_data.csv', index = None, header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    return training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}