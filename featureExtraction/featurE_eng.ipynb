{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # put up top\n",
    "#nltk.download('vader_lexicon') -- if first time running uncomment and run this\n",
    "import pandas as pd\n",
    "STOP_WORDS = set(stopwords.words('english')) # when german data, use 'german' as parameter\n",
    "new_stopwords = ['rt'] # when german data take out 'lbr'\n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# functions for filtering through rows\n",
    "def doc_generator(filepath, textcol=0, skipheader=True): # might want to make false?\n",
    "    with open(filepath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        if skipheader:\n",
    "            next(reader, None)\n",
    "        for row in reader:\n",
    "            yield row[textcol]\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = CountVectorizer(ngram_range=(min_n, max_n), stop_words=set(STOP_WORDS), max_features=10000)\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col)) # for our purposes col=0\n",
    "    #print(\"Testing ngram generation for \" + str)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    #print(\"Testing ngram vectors\" + str)\n",
    "    #print(X.toarray())\n",
    "    ngrams_pd = pd.DataFrame(X.toarray())\n",
    "    return ngrams_pd\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=10000)\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col))\n",
    "    # Testing the TFIDF value + ngrams feature names:\n",
    "    #print(X.toarray()) \n",
    "    #print(vectorizer.get_feature_names())\n",
    "    tfidf_pd = pd.DataFrame(X.toarray())\n",
    "    return tfidf_pd\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(str, data):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    data_pd = data.as_data_frame()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(data_pd)-1):\n",
    "        tweet = data_pd['cleaned_tweet'].values[i]\n",
    "        ss = sid.polarity_scores(tweet)\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"11.0.2\" 2019-01-15 LTS; Java(TM) SE Runtime Environment 18.9 (build 11.0.2+9-LTS); Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.2+9-LTS, mixed mode)\n",
      "  Starting server from /home/mackenzie/anaconda3/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpb7xqwpfe\n",
      "  JVM stdout: /tmp/tmpb7xqwpfe/h2o_mackenzie_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpb7xqwpfe/h2o_mackenzie_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Vienna</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>21 days, 15 hours and 20 minutes </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mackenzie_smvxwt</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.922 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Europe/Vienna\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    21 days, 15 hours and 20 minutes\n",
       "H2O cluster name:           H2O_from_python_mackenzie_smvxwt\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.922 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.1 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# prepare data for h2o use, check if paths are correct\n",
    "filepath_train = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTrainingData (1).csv\"\n",
    "filepath_test = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTestingData (1).csv\"\n",
    "train_data = h2o.upload_file(filepath_train) \n",
    "test_data = h2o.upload_file(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break train into sequence of words\n"
     ]
    },
    {
     "ename": "H2OResponseError",
     "evalue": "Server error java.lang.IllegalArgumentException:\n  Error: Column cleaned_tweet not found\n  Request: POST /99/Rapids\n    data: {'ast': \"(tmp= py_1_sid_a5b1 (cols_py Key_Frame__upload_82faca47600d6d4efd422bb96bc4e88.hex 'cleaned_tweet'))\", 'session_id': '_sid_a5b1'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OResponseError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b315cb7c6b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word2Vec generation resulting in pandas data frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvecs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh2o_w2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvecs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh2o_w2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bb207e06df68>\u001b[0m in \u001b[0;36mh2o_w2vec\u001b[0;34m(data, str)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mh2o_w2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Break \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" into sequence of words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizeFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Build word2vec model for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH2OWord2vecEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_sample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bb207e06df68>\u001b[0m in \u001b[0;36mtokenizeFunc\u001b[0;34m(sentences, stop_word)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# functions for H2o Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizeFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH2OFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36mas_data_frame\u001b[0;34m(self, use_pandas, header)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcan_use_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_pandas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36mget_frame_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mindividual\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mseparated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mcommas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[0;32m-> 1315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET /3/DownloadDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"frame_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hex_string\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36mframe_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mframe_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;34m\"\"\"Internal id of the frame (str).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36m_frame\u001b[0;34m(self, rows, rows_offset, cols, cols_offset, fill_cache)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfill_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/expr.py\u001b[0m in \u001b[0;36m_eager_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m  \u001b[0;31m# Data already computed under ID, but not cached locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eager_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# returns a scalar (or a list of scalars)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/expr.py\u001b[0m in \u001b[0;36m_eval_driver\u001b[0;34m(self, top)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mexec_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ast_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExprNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrapids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'scalar'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scalar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/expr.py\u001b[0m in \u001b[0;36mrapids\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpython\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mRapids\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST /99/Rapids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ast\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"session_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/h2o.py\u001b[0m in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# type checks are performed in H2OConnection class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0m_check_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mh2oconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    405\u001b[0m                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_end_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m412\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH2OErrorV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2OModelBuilderErrorV3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mH2OResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;31m# Server errors (notably 500 = \"Server Error\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OResponseError\u001b[0m: Server error java.lang.IllegalArgumentException:\n  Error: Column cleaned_tweet not found\n  Request: POST /99/Rapids\n    data: {'ast': \"(tmp= py_1_sid_a5b1 (cols_py Key_Frame__upload_82faca47600d6d4efd422bb96bc4e88.hex 'cleaned_tweet'))\", 'session_id': '_sid_a5b1'}\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data['cleaned_tweet'], 'train')).as_data_frame()\n",
    "train_labels = train_data[\"labels\"].as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data['cleaned_tweet'], 'test')).as_data_frame()\n",
    "test_labels = test_data[\"labels\"].as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train_freq = ngrams(1, 3, 'train', filepath_train, 0) \n",
    "ngram_test_freq = ngrams(1, 3, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(1, 3, 'train', filepath_train, 0) \n",
    "tfidf_test = tfidf(1, 3, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer('train', train_data['cleaned_tweet']) \n",
    "sentiment_test = sentimentAnalyzer('test', test_data['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: figure out why mem error for when running training data\n",
    "\n",
    "# combine all features into one pandas dataframe for train and test\n",
    "training_data = pd.concat([vecs_train, ngram_train_freq, tfidf_train, sentiment_train, train_labels])\n",
    "export_csv = training_data.to_csv('english_train_data.csv', index = None, header=True, encoding='utf-8')\n",
    "\n",
    "#testing_data = pd.concat([vecs_test, ngram_test_freq, tfidf_test, sentiment_test, test_labels]) \n",
    "#export_csv2 = testing_data.to_csv('english_test_data.csv', index = None, header=True, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
