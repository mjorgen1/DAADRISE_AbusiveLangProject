{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textstat\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9ebab9e0813a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Preparations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/mackenzie/Downloads/slang.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n\u001b[0;32m      4\u001b[0m     for line in file if line.strip())\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mackenzie/Downloads/slang.txt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mackenzie/Downloads/slang.txt'",
     "output_type": "error"
    }
   ],
   "source": [
    "# Preparations\n",
    "with open('/home/mackenzie/Downloads/slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "new_stopwords = ['rt'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    return bv_data\n",
    "\n",
    "# Char-NGrams Freq function\n",
    "def char_ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    return tv_data\n",
    "\n",
    "# Char-TFIDF function\n",
    "def char_tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    return tv_data\n",
    "\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def is_retweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if 'RT' in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_elongated(text):\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def number_of_slangs(text):\n",
    "    slang_counter = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slang_counter += 1\n",
    "    return slang_counter\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    return len(re.findall(r\"&#\\S+\", text))\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, irt, nom, noh, nol, noem, nt, noel, nos = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        irt.append(is_retweet(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "        noel.append(number_of_elongated(tweets[i]))\n",
    "        nos.append(number_of_slangs(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['retweet'] = irt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of elongated'] = noel\n",
    "    features['number of slangs'] = nos\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Textstat score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def textstatScores(tweets, str):\n",
    "    print(\"Completing the text stat scores extraction for \" + str)\n",
    "    fre, fkg, fs, si, ari, cli, lwf, dcrs = [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        fre.append(textstat.flesch_reading_ease(tweet))\n",
    "        fkg.append(textstat.flesch_kincaid_grade(tweet))\n",
    "        fs.append(textstat.gunning_fog(tweet))\n",
    "        si.append(textstat.smog_index(tweet))\n",
    "        ari.append(textstat.automated_readability_index(tweet))\n",
    "        cli.append(textstat.coleman_liau_index(tweet))\n",
    "        lwf.append(textstat.linsear_write_formula(tweet))\n",
    "        dcrs.append(textstat.dale_chall_readability_score(tweet))\n",
    "    features = pd.DataFrame()\n",
    "    features['flesch reading ease'] = fre\n",
    "    features['flesch kincaid grade'] = fkg\n",
    "    features['gunning fog'] = fs\n",
    "    features['smog index'] = si\n",
    "    features['automated readability index'] = ari\n",
    "    features['coleman liau index'] = cli\n",
    "    features['linsear write formula'] = lwf\n",
    "    features['dale chall readability score'] = dcrs\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"11.0.2\" 2019-01-15 LTS; Java(TM) SE Runtime Environment 18.9 (build 11.0.2+9-LTS); Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.2+9-LTS, mixed mode)\n",
      "  Starting server from /home/mackenzie/anaconda3/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpu5tl7jg8\n",
      "  JVM stdout: /tmp/tmpu5tl7jg8/h2o_mackenzie_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpu5tl7jg8/h2o_mackenzie_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Vienna</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>23 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mackenzie_79hb59</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.922 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Europe/Vienna\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    23 days\n",
       "H2O cluster name:           H2O_from_python_mackenzie_79hb59\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.922 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.1 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_train = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTrainingData (1).csv\"\n",
    "filepath_test = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/EnglishCleanedTestingData (1).csv\"\n",
    "train_data_h2o = h2o.upload_file(filepath_train) \n",
    "test_data_h2o = h2o.upload_file(filepath_test)\n",
    "train_data = pd.read_csv(filepath_train)\n",
    "test_data = pd.read_csv(filepath_test)\n",
    "train_labels = train_data[\"labels\"]\n",
    "test_labels = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break train into sequence of words\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Build word2vec model for train\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n",
      "Break test into sequence of words\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Build word2vec model for test\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data_h2o['cleaned_tweet'], 'train')).as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data_h2o['cleaned_tweet'], 'test')).as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing ngram generation for train\n",
      "Completing ngram generation for test\n",
      "Completing ngram generation for train\n",
      "Completing ngram generation for test\n"
     ]
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train = ngrams(train_data['cleaned_tweet'], 1, 3, 'train') \n",
    "ngram_test = ngrams(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# Char-NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "char_ngram_train = char_ngrams(train_data['cleaned_tweet'], 2, 5, 'train') \n",
    "char_ngram_test = char_ngrams(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n",
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(train_data['cleaned_tweet'], 1, 3, 'train')\n",
    "tfidf_test = tfidf(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# TFIDF Generation -- NOTE no header column\n",
    "char_tfidf_train = char_tfidf(train_data['cleaned_tweet'], 2, 5, 'train')\n",
    "char_tfidf_test = char_tfidf(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the sentiment analysis for train\n",
      "Completing the sentiment analysis for test\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer(train_data['cleaned_tweet'], 'train') \n",
    "sentiment_test = sentimentAnalyzer(test_data['cleaned_tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the liguistic feature extraction for train\n",
      "Completing the liguistic feature extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_train = linguisticFeatures(train_data['tweet'], 'train')\n",
    "linguistic_test = linguisticFeatures(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the text stat scores extraction for train\n",
      "Completing the text stat scores extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Textstat scores\n",
    "textstat_train = textstatScores(train_data['tweet'], 'train')\n",
    "textstat_test = textstatScores(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating in process\n",
      "Did it!\n"
     ]
    }
   ],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "training_data = pd.concat([ngram_train, char_ngram_train, tfidf_train, char_tfidf_train, sentiment_train, \n",
    "                           linguistic_train, textstat_train, train_labels], axis=1) # took out vecs_train\n",
    "print(\"Did it!\")\n",
    "\n",
    "#testing_data = pd.concat([vecs_test, ngram_test, tfidf_test, sentiment_test, test_labels], axis=1) \n",
    "#export_csv2 = testing_data.to_csv('english_test_data.csv', index = None, header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    return training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}