{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import readability\n",
    "import os\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" This is for h2o word2vec!!\\n# Prepare stopwords, rt is refers to ReTweet\\nSTOP_WORDS = set(stopwords.words('english'))\\nnew_stopwords = ['rt'] \\nSTOP_WORDS = STOP_WORDS.union(new_stopwords)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparations\n",
    "# Change directory path to get the file\n",
    "#os.chdir('/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/')\n",
    "os.chdir('C:\\\\Users\\\\mikec\\\\Documents')\n",
    "# Opening the slang text file for counting number of slang words\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "slang_words = sorted(slang_map, key=len, reverse=True)\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "''' This is for h2o word2vec!!\n",
    "# Prepare stopwords, rt is refers to ReTweet\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "new_stopwords = ['rt'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "''' This word2vec includes nan values.\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "'''\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [col + '_nw' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# Char-NGrams Freq function\n",
    "def char_ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing char-ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [col + '_nc' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col + '_tw' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "# Char-TFIDF function\n",
    "def char_tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing char-tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col + '_tc' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def is_retweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if 'RT' in tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_elongated(text):\n",
    "    regex = re.compile(r\"(.)\\1{2}\")\n",
    "    return len([word for word in text.split() if regex.search(word)])\n",
    "\n",
    "def number_of_slangs(text):\n",
    "    slang_counter = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in slang_words:\n",
    "            slang_counter += 1\n",
    "    return slang_counter\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    return len(re.findall(r\"&#\\S+\", text))\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, irt, nom, noh, nol, noem, nt, noel, nos = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        irt.append(is_retweet(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "        noel.append(number_of_elongated(tweets[i]))\n",
    "        nos.append(number_of_slangs(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['retweet'] = irt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of elongated'] = noel\n",
    "    features['number of slangs'] = nos\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Readability score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    text = re.sub(r'\\&#S+', '', text)\n",
    "    return text\n",
    "\n",
    "def readabilityScores(tweets, str):\n",
    "    print(\"Completing the readability scores extraction for \" + str)\n",
    "    fkg, ari, cli, fre, gfi, lix, si, rix, dci = [], [], [], [], [], [], [], [], []\n",
    "    cpw, spw, wps, ttr, c, s, w, wt, lw, cw, cwdc = [], [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        # Readability\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        tweet = remove_emojis(tweet)\n",
    "        measures = readability.getmeasures(tweet, lang='en')\n",
    "        fkg.append(measures['readability grades']['Kincaid'])\n",
    "        ari.append(measures['readability grades']['ARI'])\n",
    "        cli.append(measures['readability grades']['Coleman-Liau'])\n",
    "        fre.append(measures['readability grades']['FleschReadingEase'])\n",
    "        gfi.append(measures['readability grades']['GunningFogIndex'])\n",
    "        lix.append(measures['readability grades']['LIX'])\n",
    "        si.append(measures['readability grades']['SMOGIndex'])\n",
    "        rix.append(measures['readability grades']['RIX'])\n",
    "        dci.append(measures['readability grades']['DaleChallIndex'])\n",
    "        # Sentence\n",
    "        cpw.append(measures['sentence info']['characters_per_word'])\n",
    "        spw.append(measures['sentence info']['syll_per_word'])\n",
    "        wps.append(measures['sentence info']['words_per_sentence'])\n",
    "        ttr.append(measures['sentence info']['type_token_ratio'])\n",
    "        c.append(measures['sentence info']['characters'])\n",
    "        s.append(measures['sentence info']['syllables'])\n",
    "        w.append(measures['sentence info']['words'])\n",
    "        wt.append(measures['sentence info']['wordtypes'])\n",
    "        lw.append(measures['sentence info']['long_words'])\n",
    "        cw.append(measures['sentence info']['complex_words'])\n",
    "        cwdc.append(measures['sentence info']['complex_words_dc'])\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    # Readability\n",
    "    features['Kincaid'] = fkg\n",
    "    features['ARI'] = ari\n",
    "    features['Coleman-Liau'] = cli\n",
    "    features['FleschReadingEase'] = fre\n",
    "    features['GunningFogIndex'] = gfi\n",
    "    features['LIX'] = lix\n",
    "    features['SMOGIndex'] = si\n",
    "    features['RIX'] = rix\n",
    "    features['DaleChallIndex'] = dci\n",
    "    # Sentence\n",
    "    features['Characters per word'] = cpw\n",
    "    features['Syllables per word'] = spw\n",
    "    features['Words per sentence'] = wps\n",
    "    features['Type toke ratio'] = ttr\n",
    "    features['Characters'] = c\n",
    "    features['Syllables'] = s\n",
    "    features['Words'] = w\n",
    "    features['Wordtypes'] = wt\n",
    "    features['Long words'] = lw\n",
    "    features['Complex words'] = cw\n",
    "    features['Complex words dc'] = cwdc\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create an h2o instance\n",
    "#h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_train = \"EnglishCleanedTrainingData.csv\"\n",
    "filepath_test = \"EnglishCleanedTestingData.csv\"\n",
    "\n",
    "#train_data_h2o = h2o.upload_file(filepath_train) \n",
    "#test_data_h2o = h2o.upload_file(filepath_test)\n",
    "\n",
    "train_data = pd.read_csv(filepath_train)\n",
    "test_data = pd.read_csv(filepath_test)\n",
    "train_labels = train_data[\"labels\"]\n",
    "test_labels = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "#vecs_train = (h2o_w2vec(train_data_h2o['cleaned_tweet'], 'train')).as_data_frame()\n",
    "#vecs_test = (h2o_w2vec(test_data_h2o['cleaned_tweet'], 'test')).as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing ngram generation for train\n",
      "Completing ngram generation for test\n",
      "Completing char-ngram generation for train\n",
      "Completing char-ngram generation for test\n"
     ]
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train = ngrams(train_data['cleaned_tweet'], 1, 3, 'train') \n",
    "ngram_test = ngrams(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# Char-NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "char_ngram_train = char_ngrams(train_data['cleaned_tweet'], 2, 5, 'train') \n",
    "char_ngram_test = char_ngrams(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n",
      "Completing char-tfidf+ngram generation for train\n",
      "Completing char-tfidf+ngram generation for test\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(train_data['cleaned_tweet'], 1, 3, 'train')\n",
    "tfidf_test = tfidf(test_data['cleaned_tweet'], 1, 3, 'test')\n",
    "# TFIDF Generation -- NOTE no header column\n",
    "char_tfidf_train = char_tfidf(train_data['cleaned_tweet'], 2, 5, 'train')\n",
    "char_tfidf_test = char_tfidf(test_data['cleaned_tweet'], 2, 5, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the sentiment analysis for train\n",
      "Completing the sentiment analysis for test\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer(train_data['cleaned_tweet'], 'train') \n",
    "sentiment_test = sentimentAnalyzer(test_data['cleaned_tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the liguistic feature extraction for train\n",
      "Completing the liguistic feature extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_train = linguisticFeatures(train_data['tweet'], 'train')\n",
    "linguistic_test = linguisticFeatures(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing the readability scores extraction for train\n",
      "Completing the readability scores extraction for test\n"
     ]
    }
   ],
   "source": [
    "# Readability scores\n",
    "readability_train = readabilityScores(train_data['tweet'], 'train')\n",
    "readability_test = readabilityScores(test_data['tweet'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating in process\n",
      "Did it!\n"
     ]
    }
   ],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "training_data = pd.concat([ngram_train, char_ngram_train, tfidf_train, char_tfidf_train, sentiment_train, \n",
    "                           linguistic_train, readability_train, train_labels], axis=1)\n",
    "\n",
    "testing_data = pd.concat([ngram_test, char_ngram_test, tfidf_test, char_tfidf_test, sentiment_test, \n",
    "                           linguistic_test, readability_test, test_labels], axis=1) \n",
    "print(\"Did it!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def getTrainData():\n",
    "    return training_data\n",
    "def getTestData():\n",
    "    return testing_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
