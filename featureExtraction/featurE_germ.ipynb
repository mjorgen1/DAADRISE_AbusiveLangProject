{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import readability\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Preparations\n",
    "# Change directory path to get the file\n",
    "#os.chdir('/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/')\n",
    "os.chdir('C:\\\\Users\\\\mikec\\\\Documents')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "# NGrams Freq function\n",
    "def ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [col + '_nw' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# Char-NGrams Freq function\n",
    "def char_ngrams(text, min_n, max_n, str):\n",
    "    print(\"Completing char-ngram generation for \" + str)\n",
    "    bv = CountVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    bv_matrix = bv.fit_transform(text).toarray()\n",
    "    bv_vocab = bv.get_feature_names()\n",
    "    bv_data = pd.DataFrame(bv_matrix, columns=bv_vocab)\n",
    "    bv_data.columns = [col + '_nc' for col in bv_data.columns]\n",
    "    return bv_data\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000)\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col + '_tw' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "# Char-TFIDF function\n",
    "def char_tfidf(text, min_n, max_n, str):\n",
    "    print(\"Completing char-tfidf+ngram generation for \" + str)\n",
    "    tv = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=1000, analyzer='char_wb')\n",
    "    tv_matrix = tv.fit_transform(text).toarray()\n",
    "    tv_vocab = tv.get_feature_names()\n",
    "    tv_data = pd.DataFrame(np.round(tv_matrix, 2), columns=tv_vocab)\n",
    "    tv_data.columns = [col + '_tc' for col in tv_data.columns]\n",
    "    return tv_data\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(tweets, str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(tweets)):\n",
    "        ss = sid.polarity_scores(tweets[i])\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val\n",
    "\n",
    "# Linguistic Feature Extraction \n",
    "def text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def number_of_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def number_of_mentions(text):\n",
    "    return len(re.findall(r\"@\\S+\", text))\n",
    "\n",
    "def number_of_hashtags(text):\n",
    "    return len(re.findall(r\"#\\S+\", text))\n",
    "\n",
    "def number_of_links(text):\n",
    "    return len(re.findall(r\"http\\S+\", text))\n",
    "\n",
    "def number_of_emoticons(text):\n",
    "    count = 0\n",
    "    for character in text:\n",
    "        if character in emoji.UNICODE_EMOJI:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def linguisticFeatures(tweets, str):\n",
    "    print(\"Completing the liguistic feature extraction for \" + str)\n",
    "    tl, nt, nom, noh, nol, noem = [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tl.append(text_length(tweets[i]))\n",
    "        nom.append(number_of_mentions(tweets[i]))\n",
    "        noh.append(number_of_hashtags(tweets[i]))\n",
    "        nol.append(number_of_links(tweets[i]))\n",
    "        noem.append(number_of_emoticons(tweets[i]))\n",
    "        nt.append(number_of_tokens(tweets[i]))\n",
    "    features = pd.DataFrame()\n",
    "    features['text length'] = tl\n",
    "    features['number of words'] = nt\n",
    "    features['number of mentions'] = nom\n",
    "    features['number of hashtags'] = noh\n",
    "    features['number of links'] = nol\n",
    "    features['number of emoticons'] = noem\n",
    "    return features\n",
    "\n",
    "# Readability score extraction\n",
    "def remove_user_names(text):\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_underscore(text):\n",
    "    text = text.replace('_', '')\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "def readabilityScores(tweets, str):\n",
    "    print(\"Completing the readability scores extraction for \" + str)\n",
    "    fkg, ari, cli, fre, gfi, lix, si, rix, dci = [], [], [], [], [], [], [], [], []\n",
    "    cpw, spw, wps, ttr, c, s, w, wt, lw, cw, cwdc = [], [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = remove_user_names(tweet)\n",
    "        tweet = remove_hashtags(tweet)\n",
    "        tweet = remove_links(tweet)\n",
    "        tweet = remove_underscore(tweet)\n",
    "        tweet = remove_emojis(tweet)\n",
    "        measures = readability.getmeasures(tweet, lang='de')\n",
    "        fkg.append(measures['readability grades']['Kincaid'])\n",
    "        ari.append(measures['readability grades']['ARI'])\n",
    "        cli.append(measures['readability grades']['Coleman-Liau'])\n",
    "        fre.append(measures['readability grades']['FleschReadingEase'])\n",
    "        gfi.append(measures['readability grades']['GunningFogIndex'])\n",
    "        lix.append(measures['readability grades']['LIX'])\n",
    "        si.append(measures['readability grades']['SMOGIndex'])\n",
    "        rix.append(measures['readability grades']['RIX'])\n",
    "        dci.append(measures['readability grades']['DaleChallIndex'])\n",
    "        # Sentence\n",
    "        cpw.append(measures['sentence info']['characters_per_word'])\n",
    "        spw.append(measures['sentence info']['syll_per_word'])\n",
    "        wps.append(measures['sentence info']['words_per_sentence'])\n",
    "        ttr.append(measures['sentence info']['type_token_ratio'])\n",
    "        c.append(measures['sentence info']['characters'])\n",
    "        s.append(measures['sentence info']['syllables'])\n",
    "        w.append(measures['sentence info']['words'])\n",
    "        wt.append(measures['sentence info']['wordtypes'])\n",
    "        lw.append(measures['sentence info']['long_words'])\n",
    "        cw.append(measures['sentence info']['complex_words'])\n",
    "        cwdc.append(measures['sentence info']['complex_words_dc'])\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    # Readability\n",
    "    features['Kincaid'] = fkg\n",
    "    features['ARI'] = ari\n",
    "    features['Coleman-Liau'] = cli\n",
    "    features['FleschReadingEase'] = fre\n",
    "    features['GunningFogIndex'] = gfi\n",
    "    features['LIX'] = lix\n",
    "    features['SMOGIndex'] = si\n",
    "    features['RIX'] = rix\n",
    "    features['DaleChallIndex'] = dci\n",
    "    # Sentence\n",
    "    features['Characters per word'] = cpw\n",
    "    features['Syllables per word'] = spw\n",
    "    features['Words per sentence'] = wps\n",
    "    features['Type toke ratio'] = ttr\n",
    "    features['Characters'] = c\n",
    "    features['Syllables'] = s\n",
    "    features['Words'] = w\n",
    "    features['Wordtypes'] = wt\n",
    "    features['Long words'] = lw\n",
    "    features['Complex words'] = cw\n",
    "    features['Complex words dc'] = cwdc\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# prepare data for feature extraction, check if paths are correct\n",
    "filepath_data = \"GermanCleanedData.csv\"\n",
    "data = pd.read_csv(filepath_data)\n",
    "labels = data[\"labels\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing ngram generation for data\n",
      "Completing char-ngram generation for data\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_data = ngrams(data['cleaned_tweet'], 1, 3, 'data') \n",
    "# Char-NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "char_ngram_data = char_ngrams(data['cleaned_tweet'], 2, 5, 'data') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completing tfidf+ngram generation for data\n",
      "Completing char-tfidf+ngram generation for data\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_data = tfidf(data['cleaned_tweet'], 1, 3, 'data')\n",
    "# TFIDF Generation -- NOTE no header column\n",
    "char_tfidf_data = char_tfidf(data['cleaned_tweet'], 2, 5, 'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_data = sentimentAnalyzer(data['cleaned_tweet'], 'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liguistic feature extraction\n",
    "linguistic_data = linguisticFeatures(data['tweet'], 'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Readability scores\n",
    "readability_data = readabilityScores(data['tweet'], 'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "print(\"Concatenating in process\")\n",
    "featured_data = pd.concat([ngram_data, char_ngram_data, tfidf_data, char_tfidf_data, sentiment_data, \n",
    "                           linguistic_data, readability_data, labels], axis=1)\n",
    "print(\"Finished it!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data set into three data sets based on the labels\n",
    "for labels, d in featured_data.groupby('labels'):\n",
    "    globals()['data_' + str(labels)] = d\n",
    "del d\n",
    "\n",
    "# Find the 80% cut-line for each data set\n",
    "cut_0 = round(len(data_0.index) * 0.8)\n",
    "cut_1 = round(len(data_1.index) * 0.8)\n",
    "cut_2 = round(len(data_2.index) * 0.8)\n",
    "\n",
    "# Construct train and test data sets\n",
    "train = pd.concat([data_0.iloc[:cut_0, :], data_1.iloc[:cut_1, :], data_2.iloc[:cut_2, :]])\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "test = pd.concat([data_0.iloc[cut_0:, :], data_1.iloc[cut_1:, :], data_2.iloc[cut_2:, :]])\n",
    "test = test.reindex(np.random.permutation(test.index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getTrainData():\n",
    "    return train\n",
    "def getTestData():\n",
    "    return test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}