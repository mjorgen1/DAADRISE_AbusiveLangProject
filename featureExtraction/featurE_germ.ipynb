{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon') -- if first time running uncomment and run this\n",
    "import pandas as pd\n",
    "STOP_WORDS = set(stopwords.words('german')) \n",
    "new_stopwords = ['lbr'] \n",
    "STOP_WORDS = STOP_WORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Feature Extraction Functions\n",
    "\n",
    "# functions for H2o Word2Vec\n",
    "def tokenizeFunc(sentences, stop_word = STOP_WORDS):\n",
    "    df = sentences.as_data_frame()\n",
    "    df = df.astype(str)\n",
    "    sentence = h2o.H2OFrame(python_obj=df, column_types=[\"string\"])\n",
    "    tokenized = sentence.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "def h2o_w2vec(data, str):\n",
    "    print(\"Break \" + str + \" into sequence of words\")\n",
    "    words = tokenizeFunc(data)\n",
    "    print(\"Build word2vec model for \" + str)\n",
    "    w2v_model = H2OWord2vecEstimator(sent_sample_rate=0.0, epochs=10)\n",
    "    w2v_model.train(training_frame=words)\n",
    "    vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "    return vecs\n",
    "\n",
    "# functions for filtering through rows\n",
    "def doc_generator(filepath, textcol=0, skipheader=True): # might want to make false?\n",
    "    with open(filepath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        if skipheader:\n",
    "            next(reader, None)\n",
    "        for row in reader:\n",
    "            yield row[textcol]\n",
    "\n",
    "# NGrams Freq function\n",
    "def ngrams(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = CountVectorizer(ngram_range=(min_n, max_n), stop_words=set(STOP_WORDS), max_features=10000)\n",
    "    print(\"Completing ngram generation for \" + str)\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col)) # for our purposes col=0\n",
    "    #print(\"Testing ngram generation for \" + str)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    #print(\"Testing ngram vectors\" + str)\n",
    "    #print(X.toarray())\n",
    "    ngrams_pd = pd.DataFrame(X.toarray())\n",
    "    return ngrams_pd\n",
    "\n",
    "# TFIDF function\n",
    "def tfidf(min_n, max_n, str, filepath, col):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(min_n, max_n), max_features=10000)\n",
    "    print(\"Completing tfidf+ngram generation for \" + str)\n",
    "    X = vectorizer.fit_transform(doc_generator(filepath, textcol=col))\n",
    "    # Testing the TFIDF value + ngrams feature names:\n",
    "    #print(X.toarray()) \n",
    "    #print(vectorizer.get_feature_names())\n",
    "    tfidf_pd = pd.DataFrame(X.toarray())\n",
    "    return tfidf_pd\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def sentimentAnalyzer(str, data):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    data_pd = data.as_data_frame()\n",
    "    sentiment_val = pd.DataFrame(columns = ['sentiment'])\n",
    "    print(\"Completing the sentiment analysis for \" + str)\n",
    "    for i in range(0, len(data_pd)-1):\n",
    "        tweet = data_pd['cleaned_tweet'].values[i]\n",
    "        ss = sid.polarity_scores(tweet)\n",
    "        sentiment_val.at[i, 'sentiment'] = ss.get('compound')\n",
    "    return sentiment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create an h2o instance\n",
    "h2o.init()\n",
    "#nltk.download('stopwords')  # might need if running nltk + stopwords for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for h2o use, check if paths are correct\n",
    "filepath_train = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/GermanCleanedTrainingData (1).csv\"\n",
    "filepath_test = \"/home/mackenzie/workspace/PycharmProjects/DAADRISE_AbusiveLangProject/featureExtraction/GermanCleanedTestingData (1).csv\"\n",
    "train_data = h2o.upload_file(filepath_train) \n",
    "test_data = h2o.upload_file(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec generation resulting in pandas data frames\n",
    "vecs_train = (h2o_w2vec(train_data['cleaned_tweet'], 'train')).as_data_frame()\n",
    "train_labels = train_data[\"labels\"].as_data_frame()\n",
    "vecs_test = (h2o_w2vec(test_data['cleaned_tweet'], 'test')).as_data_frame()\n",
    "test_labels = test_data[\"labels\"].as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGRAMS generation + Frequency calculation -- NOTE no header column\n",
    "ngram_train_freq = ngrams(1, 3, 'train', filepath_train, 0) # unigram for now\n",
    "ngram_test_freq = ngrams(1, 3, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Generation -- NOTE no header column\n",
    "tfidf_train = tfidf(1, 3, 'train', filepath_train, 0) # unigram for now\n",
    "tfidf_test = tfidf(1, 3, 'test', filepath_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis into pandas dataframes\n",
    "sentiment_train = sentimentAnalyzer('train', train_data['cleaned_tweet']) \n",
    "sentiment_test = sentimentAnalyzer('test', test_data['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all features into one pandas dataframe for train and test\n",
    "training_data = pd.concat([vecs_train, ngram_train_freq, tfidf_train, sentiment_train, train_labels])\n",
    "export_csv = training_data.to_csv('german_train_data.csv', index = None, header=True, encoding='utf-8')\n",
    "\n",
    "testing_data = pd.concat([vecs_test, ngram_test_freq, tfidf_test, sentiment_test, test_labels]) \n",
    "export_csv2 = testing_data.to_csv('german_test_data.csv', index = None, header=True, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
