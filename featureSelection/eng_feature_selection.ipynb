{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\mikec\\\\Codes\\\\DAADRISE_AbusiveLangProject\\\\featureExtraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from featurE_eng.ipynb\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) Client VM (build 25.211-b12, mixed mode)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikec\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\h2o\\backend\\server.py:369: UserWarning:   You have a 32-bit version of Java. H2O works best with 64-bit Java.\n",
      "  Please download the latest 64-bit Java SE JDK from Oracle.\n",
      "\n",
      "  warn(\"  You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Starting server from C:\\Users\\mikec\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\mikec\\AppData\\Local\\Temp\\tmpb1iinjwi\n",
      "  JVM stdout: C:\\Users\\mikec\\AppData\\Local\\Temp\\tmpb1iinjwi\\h2o_mikec_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\mikec\\AppData\\Local\\Temp\\tmpb1iinjwi\\h2o_mikec_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>26 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mikec_5oq0zc</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>247.5 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Europe/Berlin\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    26 days\n",
       "H2O cluster name:           H2O_from_python_mikec_5oq0zc\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    247.5 Mb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.3 final\n",
       "--------------------------  ------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Break train into sequence of words\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Build word2vec model for train\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n",
      "Break test into sequence of words\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Build word2vec model for test\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n",
      "Completing ngram generation for train\n",
      "Completing ngram generation for test\n",
      "Completing ngram generation for train\n",
      "Completing ngram generation for test\n",
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n",
      "Completing tfidf+ngram generation for train\n",
      "Completing tfidf+ngram generation for test\n",
      "Completing the sentiment analysis for train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import import_ipynb\n",
    "from featurE_eng import getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the features from the featureExtraction file\n",
    "data = getData()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the features and target variable\n",
    "X = data.iloc[:,0:4018]  #independent columns, change column number depending on what we have for shape\n",
    "y = data.iloc[:,-1]    #target column i.e labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if nan values check\n",
    "#print(np.where(np.isnan(X)))\n",
    "#print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Selection -- apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=100)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "# concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print('Univariate Selection results:')\n",
    "print(featureScores.nlargest(100,'Score'))  #print 100 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X,y)\n",
    "print('Feature Importance results:')\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(feat_importances.nlargest(100))\n",
    "\n",
    "#feat_importances.nlargest(100).plot(kind='barh')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix with Heatmap -- get correlations of each features in dataset\n",
    "corrmat = data.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}